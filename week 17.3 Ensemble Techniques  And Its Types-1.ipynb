{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ef23ce1-d87e-406c-9325-b57563c0f9cf",
   "metadata": {},
   "source": [
    "**Q1. What is an ensemble technique in machine learning?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfd94b0-a8db-49a9-a209-7818be244f9b",
   "metadata": {},
   "source": [
    "**ANSWER:---------**\n",
    "\n",
    "\n",
    "Ensemble techniques in machine learning involve combining the predictions of multiple models to improve the overall performance and robustness of the final model. The basic idea is that by leveraging the strengths and compensating for the weaknesses of different models, the ensemble can achieve better accuracy and generalization than any single model.\n",
    "\n",
    "### Types of Ensemble Techniques\n",
    "\n",
    "1. **Bagging (Bootstrap Aggregating):**\n",
    "   - **Description:** In bagging, multiple models (typically of the same type) are trained on different subsets of the training data, which are created by sampling with replacement (bootstrap sampling).\n",
    "   - **Example:** Random Forest is a popular bagging algorithm that builds a large number of decision trees and combines their outputs.\n",
    "\n",
    "2. **Boosting:**\n",
    "   - **Description:** Boosting involves sequentially training models, where each subsequent model focuses on correcting the errors made by the previous ones. The models are combined to make a final prediction.\n",
    "   - **Example:** Gradient Boosting, AdaBoost, and XGBoost are widely used boosting algorithms.\n",
    "\n",
    "3. **Stacking (Stacked Generalization):**\n",
    "   - **Description:** Stacking involves training multiple different models and then using another model (called a meta-model or a second-level model) to learn how to best combine the predictions of the base models.\n",
    "   - **Example:** A typical stacking ensemble might combine logistic regression, decision trees, and support vector machines, using a logistic regression model to combine their predictions.\n",
    "\n",
    "4. **Voting:**\n",
    "   - **Description:** Voting is a simple ensemble technique where multiple models are trained independently and their predictions are combined through a voting mechanism.\n",
    "   - **Types:**\n",
    "     - **Hard Voting:** The final prediction is based on the majority vote of the individual models.\n",
    "     - **Soft Voting:** The final prediction is based on the average of the predicted probabilities from each model.\n",
    "\n",
    "### Advantages of Ensemble Techniques\n",
    "\n",
    "- **Improved Accuracy:** By combining multiple models, ensemble techniques often achieve higher accuracy than any individual model.\n",
    "- **Robustness:** Ensembles can reduce the risk of overfitting and improve the generalization of the model.\n",
    "- **Error Reduction:** Different models may make different errors, so combining them can help reduce the overall error rate.\n",
    "\n",
    "### Disadvantages of Ensemble Techniques\n",
    "\n",
    "- **Complexity:** Ensembles can be more complex and computationally expensive to train and maintain.\n",
    "- **Interpretability:** The combined predictions of multiple models can be harder to interpret compared to a single model.\n",
    "\n",
    "Ensemble techniques are powerful tools in a data scientist's toolkit and are widely used in various machine learning competitions and real-world applications for their ability to boost predictive performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b89be3a-d0f4-416d-b13c-a019512a10a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd07532c-af0e-4a34-ba05-66c7c3d73e64",
   "metadata": {},
   "source": [
    "**Q2. Why are ensemble techniques used in machine learning?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d29111-38b3-4c0a-8a3f-8ce44c6b4222",
   "metadata": {},
   "source": [
    "**ANSWER:--------**\n",
    "\n",
    "\n",
    "Ensemble techniques are used in machine learning for several important reasons, primarily related to improving the performance, robustness, and reliability of predictive models. Here are some key reasons why ensemble techniques are widely employed:\n",
    "\n",
    "### 1. **Improved Accuracy**\n",
    "   - **Combining Strengths:** By combining the predictions of multiple models, ensemble techniques leverage the strengths of each model while compensating for their individual weaknesses, often leading to higher accuracy than any single model.\n",
    "   - **Error Reduction:** Different models may make different errors on the same data points. By averaging or voting on their predictions, ensemble methods can reduce the overall error rate.\n",
    "\n",
    "### 2. **Robustness and Stability**\n",
    "   - **Reducing Overfitting:** Ensembles can help mitigate overfitting, especially when individual models are prone to overfitting the training data. The averaging effect of ensemble methods tends to produce more stable and generalized predictions.\n",
    "   - **Handling Variability:** Ensemble methods are less sensitive to the peculiarities of individual datasets and random variations, resulting in more consistent performance across different datasets.\n",
    "\n",
    "### 3. **Better Generalization**\n",
    "   - **Combining Diverse Models:** Ensembles can combine models of different types (e.g., decision trees, logistic regression, support vector machines), each of which captures different aspects of the data. This diversity helps the ensemble generalize better to new, unseen data.\n",
    "   - **Robust Performance:** Because they aggregate multiple models, ensembles are better at capturing the underlying patterns in the data, leading to improved generalization.\n",
    "\n",
    "### 4. **Reduction of Bias and Variance**\n",
    "   - **Bias-Variance Trade-off:** Ensemble methods can effectively manage the trade-off between bias and variance. Techniques like bagging primarily reduce variance, while boosting can reduce both bias and variance, depending on the specific algorithm used.\n",
    "   - **Balancing Errors:** By balancing out the errors made by individual models, ensembles can provide a better overall prediction performance.\n",
    "\n",
    "### 5. **Flexibility and Versatility**\n",
    "   - **Adaptability:** Ensemble methods are flexible and can be applied to a wide range of machine learning algorithms, making them versatile tools in a data scientist’s arsenal.\n",
    "   - **Complex Problem Solving:** Ensembles are particularly useful for solving complex problems where no single model performs optimally across all aspects of the data.\n",
    "\n",
    "### 6. **Winning Strategy in Competitions**\n",
    "   - **Proven Success:** Ensembles are a common strategy among winners of machine learning competitions (e.g., Kaggle) because they consistently deliver superior performance compared to individual models.\n",
    "\n",
    "### 7. **Noise Reduction**\n",
    "   - **Mitigating Noise Impact:** By averaging the predictions of multiple models, ensembles can reduce the impact of noise in the data, leading to more reliable predictions.\n",
    "\n",
    "### Common Ensemble Techniques\n",
    "- **Bagging (Bootstrap Aggregating):** Reduces variance by training multiple models on different subsets of the data and averaging their predictions.\n",
    "- **Boosting:** Sequentially trains models, with each new model focusing on the errors of the previous ones, improving both bias and variance.\n",
    "- **Stacking:** Combines multiple different models and uses a meta-model to learn the best way to combine their predictions.\n",
    "- **Voting:** Aggregates predictions from multiple models through majority voting (hard voting) or averaging probabilities (soft voting).\n",
    "\n",
    "Ensemble techniques are powerful tools that enhance the performance and robustness of machine learning models, making them a key component in the toolkit of data scientists and machine learning practitioners."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c74155b-0552-4012-af17-869e9aff7709",
   "metadata": {},
   "source": [
    "**Q3. What is bagging?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027750c8-5244-4daa-a993-affc62872dbd",
   "metadata": {},
   "source": [
    "**ANSWER:--------**\n",
    "\n",
    "\n",
    "Bagging, short for Bootstrap Aggregating, is an ensemble technique in machine learning designed to improve the stability and accuracy of machine learning algorithms. It does this by reducing variance and helping to avoid overfitting. Bagging is particularly effective for models that are prone to high variance, such as decision trees.\n",
    "\n",
    "### How Bagging Works\n",
    "\n",
    "1. **Bootstrap Sampling:**\n",
    "   - Bagging involves generating multiple subsets of the original training dataset by randomly sampling with replacement. This means some samples may appear multiple times in a subset, while others may not appear at all.\n",
    "   - Each subset, called a bootstrap sample, is typically the same size as the original dataset but contains different instances due to the random sampling process.\n",
    "\n",
    "2. **Training Multiple Models:**\n",
    "   - For each bootstrap sample, a separate model (often of the same type) is trained independently. For example, if bagging is applied to decision trees, multiple decision trees are trained, each on a different bootstrap sample.\n",
    "\n",
    "3. **Aggregating Predictions:**\n",
    "   - Once all models are trained, their predictions are combined to produce the final output.\n",
    "   - For regression tasks, the average of the predictions from all models is taken.\n",
    "   - For classification tasks, the mode (majority vote) of the predictions is taken.\n",
    "\n",
    "### Benefits of Bagging\n",
    "\n",
    "1. **Reduction of Variance:**\n",
    "   - By averaging the predictions of multiple models, bagging reduces the variance of the final prediction. This makes the model more robust and less sensitive to fluctuations in the training data.\n",
    "\n",
    "2. **Improved Accuracy:**\n",
    "   - Bagging often leads to improved prediction accuracy compared to a single model, particularly for models that have high variance but low bias.\n",
    "\n",
    "3. **Resistance to Overfitting:**\n",
    "   - Since each model is trained on a different subset of the data, the ensemble is less likely to overfit compared to a single model trained on the entire dataset.\n",
    "\n",
    "### Example: Random Forest\n",
    "\n",
    "- **Random Forest** is a well-known example of a bagging algorithm. It constructs multiple decision trees during training and outputs the average prediction (regression) or majority vote (classification) of the individual trees.\n",
    "- In addition to bagging, Random Forests introduce additional randomness by selecting a random subset of features for each split in the trees, further enhancing diversity among the trees and improving performance.\n",
    "\n",
    "\n",
    "### Summary\n",
    "\n",
    "Bagging is a powerful ensemble technique that enhances the performance and stability of machine learning models by combining the predictions of multiple models trained on different subsets of the data. It is particularly effective for high-variance models and forms the basis for algorithms like Random Forests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69ee67ff-6c6a-49a6-a773-486c63c97b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 95.00%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Generate a synthetic dataset\n",
    "X, y = make_classification(n_samples=100, n_features=20, n_informative=15, n_redundant=5, random_state=42)\n",
    "y = np.where(y == 0, -1, 1)  # Convert labels to {-1, 1} for AdaBoost\n",
    "\n",
    "# Number of iterations\n",
    "N = 50\n",
    "\n",
    "# Initialize weights\n",
    "n_samples = X.shape[0]\n",
    "weights = np.ones(n_samples) / n_samples\n",
    "\n",
    "# Initialize a list to store the weak learners and their weights\n",
    "learners = []\n",
    "learners_weights = []\n",
    "\n",
    "# AdaBoost algorithm\n",
    "for i in range(N):\n",
    "    # Train weak learner\n",
    "    learner = DecisionTreeClassifier(max_depth=1)  # Using decision tree stump as weak learner\n",
    "    learner.fit(X, y, sample_weight=weights)\n",
    "    \n",
    "    # Predict on training data\n",
    "    predictions = learner.predict(X)\n",
    "    \n",
    "    # Calculate weighted error rate\n",
    "    misclassified = (predictions != y)\n",
    "    error = np.sum(weights * misclassified) / np.sum(weights)\n",
    "    \n",
    "    # Calculate learner's weight\n",
    "    learner_weight = np.log((1 - error) / (error + 1e-10)) / 2\n",
    "    \n",
    "    # Update weights\n",
    "    weights *= np.exp(learner_weight * misclassified)\n",
    "    weights /= np.sum(weights)  # Normalize weights\n",
    "    \n",
    "    # Save learner and its weight\n",
    "    learners.append(learner)\n",
    "    learners_weights.append(learner_weight)\n",
    "\n",
    "# Final prediction function\n",
    "def predict(X):\n",
    "    final_predictions = np.zeros(X.shape[0])\n",
    "    for learner, weight in zip(learners, learners_weights):\n",
    "        final_predictions += weight * learner.predict(X)\n",
    "    return np.sign(final_predictions)\n",
    "\n",
    "# Evaluate on training data\n",
    "final_predictions = predict(X)\n",
    "accuracy = accuracy_score(y, final_predictions)\n",
    "print(f\"Training accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6abaec-6f65-49ea-bd4c-d06fd9a66215",
   "metadata": {},
   "source": [
    "**Q4. What is boosting?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80284be-95e4-4f05-aee6-7383454f9e7e",
   "metadata": {},
   "source": [
    "**ANSWER:--------**\n",
    "\n",
    "\n",
    "Boosting is an ensemble technique in machine learning that aims to improve the accuracy of models by sequentially training a series of weak learners (models) in such a way that each subsequent model focuses on correcting the errors made by its predecessor. Unlike bagging, which trains models independently, boosting builds models iteratively, with each model attempting to address the weaknesses of the combined ensemble of all previous models.\n",
    "\n",
    "### How Boosting Works\n",
    "\n",
    "1. **Initialize Weights:**\n",
    "   - Each training instance is assigned an equal weight initially.\n",
    "\n",
    "2. **Train Weak Learner:**\n",
    "   - A weak learner (a model that performs slightly better than random guessing) is trained on the weighted training data.\n",
    "\n",
    "3. **Evaluate and Update Weights:**\n",
    "   - The performance of the weak learner is evaluated. Instances that are misclassified by the learner are given higher weights, while correctly classified instances are given lower weights. This focuses the next learner on the harder-to-predict instances.\n",
    "\n",
    "4. **Combine Learners:**\n",
    "   - The predictions of the weak learners are combined to form the final strong learner. The combination can be a weighted sum or vote, where the weights are determined based on the performance of each learner.\n",
    "\n",
    "5. **Repeat:**\n",
    "   - Steps 2-4 are repeated for a predefined number of iterations or until a certain performance threshold is reached.\n",
    "\n",
    "### Types of Boosting Algorithms\n",
    "\n",
    "1. **AdaBoost (Adaptive Boosting):**\n",
    "   - **Working:** AdaBoost adjusts the weights of misclassified instances in each iteration, focusing subsequent learners on those harder-to-predict instances.\n",
    "   - **Combination:** The final prediction is a weighted sum of the predictions from all the weak learners, where the weights depend on the learners' accuracy.\n",
    "   - **Application:** It is commonly used with decision trees as weak learners.\n",
    "\n",
    "2. **Gradient Boosting:**\n",
    "   - **Working:** Gradient Boosting builds learners sequentially, each new learner fitting to the residual errors (gradients) of the combined ensemble of previous learners.\n",
    "   - **Combination:** The final model is a weighted sum of all the weak learners, with weights determined by minimizing a loss function (often the mean squared error for regression or log-loss for classification).\n",
    "   - **Variants:** XGBoost, LightGBM, and CatBoost are popular implementations of gradient boosting, known for their efficiency and performance in handling large datasets and complex models.\n",
    "\n",
    "3. **LogitBoost:**\n",
    "   - **Working:** Similar to AdaBoost but optimized for logistic regression models. It focuses on minimizing the logistic loss function.\n",
    "   - **Combination:** Uses a weighted majority vote for classification tasks.\n",
    "\n",
    "### Benefits of Boosting\n",
    "\n",
    "1. **Improved Accuracy:**\n",
    "   - By focusing on the errors of previous models, boosting can significantly improve the accuracy and performance of the final model.\n",
    "\n",
    "2. **Reduction of Bias and Variance:**\n",
    "   - Boosting reduces both bias and variance, leading to better generalization to unseen data.\n",
    "\n",
    "3. **Handling Imbalanced Data:**\n",
    "   - Boosting can be particularly effective in handling imbalanced datasets, as it emphasizes the harder-to-classify instances.\n",
    "\n",
    "### Drawbacks of Boosting\n",
    "\n",
    "1. **Sensitivity to Noisy Data:**\n",
    "   - Boosting can be sensitive to noisy data and outliers since it focuses heavily on misclassified instances.\n",
    "\n",
    "2. **Computationally Intensive:**\n",
    "   - Boosting can be computationally more expensive and time-consuming than other ensemble methods like bagging.\n",
    "\n",
    "3. **Overfitting:**\n",
    "   - If not properly regularized, boosting can lead to overfitting, especially with a large number of iterations.\n",
    "\n",
    "\n",
    "### Summary\n",
    "\n",
    "Boosting is a powerful ensemble technique that iteratively trains models to correct the errors of previous models, thereby improving the overall accuracy and robustness of the final model. It is widely used in various machine learning applications, with popular algorithms like AdaBoost, Gradient Boosting, and their efficient implementations such as XGBoost, LightGBM, and CatBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60a4b63b-9a53-4cbb-82fc-de0acd872091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 95.00%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Generate a synthetic dataset\n",
    "X, y = make_classification(n_samples=100, n_features=20, n_informative=15, n_redundant=5, random_state=42)\n",
    "y = np.where(y == 0, -1, 1)  # Convert labels to {-1, 1} for AdaBoost\n",
    "\n",
    "# Number of iterations\n",
    "N = 50\n",
    "\n",
    "# Initialize weights\n",
    "n_samples = X.shape[0]\n",
    "weights = np.ones(n_samples) / n_samples\n",
    "\n",
    "# Initialize a list to store the weak learners and their weights\n",
    "learners = []\n",
    "learners_weights = []\n",
    "\n",
    "# AdaBoost algorithm\n",
    "for i in range(N):\n",
    "    # Train weak learner\n",
    "    learner = DecisionTreeClassifier(max_depth=1)  # Using decision tree stump as weak learner\n",
    "    learner.fit(X, y, sample_weight=weights)\n",
    "    \n",
    "    # Predict on training data\n",
    "    predictions = learner.predict(X)\n",
    "    \n",
    "    # Calculate weighted error rate\n",
    "    misclassified = (predictions != y)\n",
    "    error = np.sum(weights * misclassified) / np.sum(weights)\n",
    "    \n",
    "    # Calculate learner's weight\n",
    "    learner_weight = np.log((1 - error) / (error + 1e-10)) / 2\n",
    "    \n",
    "    # Update weights\n",
    "    weights *= np.exp(learner_weight * misclassified)\n",
    "    weights /= np.sum(weights)  # Normalize weights\n",
    "    \n",
    "    # Save learner and its weight\n",
    "    learners.append(learner)\n",
    "    learners_weights.append(learner_weight)\n",
    "\n",
    "# Final prediction function\n",
    "def predict(X):\n",
    "    final_predictions = np.zeros(X.shape[0])\n",
    "    for learner, weight in zip(learners, learners_weights):\n",
    "        final_predictions += weight * learner.predict(X)\n",
    "    return np.sign(final_predictions)\n",
    "\n",
    "# Evaluate on training data\n",
    "final_predictions = predict(X)\n",
    "accuracy = accuracy_score(y, final_predictions)\n",
    "print(f\"Training accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f0a60e-8cbe-48e2-8974-c941c22bb383",
   "metadata": {},
   "source": [
    "**Q5. What are the benefits of using ensemble techniques?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7ac147-4b84-4457-a173-2ccafb61ec0b",
   "metadata": {},
   "source": [
    "**ANSWER:--------**\n",
    "\n",
    "\n",
    "Ensemble techniques offer several significant benefits in machine learning, which contribute to their widespread use in various applications. Here are the primary advantages:\n",
    "\n",
    "### 1. Improved Accuracy\n",
    "- **Combining Multiple Models:** By aggregating the predictions of multiple models, ensemble techniques often achieve higher accuracy compared to individual models.\n",
    "- **Reduction of Errors:** They help in reducing errors (both bias and variance), leading to better performance on unseen data.\n",
    "\n",
    "### 2. Reduction of Overfitting\n",
    "- **Bagging Techniques:** Methods like bagging (e.g., Random Forests) reduce overfitting by training multiple models on different subsets of the data and averaging their predictions.\n",
    "- **Stability:** The variance reduction provided by bagging leads to more stable and reliable predictions.\n",
    "\n",
    "### 3. Enhanced Robustness\n",
    "- **Diverse Models:** Ensembles combine different models or the same model trained on different data, which makes the final prediction more robust to noise and anomalies.\n",
    "- **Resilience to Outliers:** Aggregating predictions helps mitigate the impact of outliers and noisy data points.\n",
    "\n",
    "### 4. Flexibility\n",
    "- **Various Methods:** There are multiple ensemble methods like bagging, boosting, stacking, etc., each with unique strengths that can be applied to different types of problems and datasets.\n",
    "- **Hybrid Models:** Ensembles can combine different types of models (e.g., decision trees, neural networks, SVMs) to leverage their strengths and compensate for individual weaknesses.\n",
    "\n",
    "### 5. Improved Generalization\n",
    "- **Bias-Variance Trade-off:** By reducing both bias (through boosting) and variance (through bagging), ensembles improve the generalization ability of the model, making it perform better on new, unseen data.\n",
    "- **Balanced Performance:** Ensembles can balance performance across various metrics, ensuring that the model is not overly optimized for a single measure of success.\n",
    "\n",
    "### 6. Handling Complex Problems\n",
    "- **Difficult Datasets:** Ensemble methods are particularly effective in handling complex problems with high-dimensional data, interactions between features, and intricate data distributions.\n",
    "- **Better Decision Boundaries:** They can create more accurate decision boundaries in classification tasks by combining the strengths of multiple models.\n",
    "\n",
    "### 7. Versatility\n",
    "- **Applicability:** Ensemble techniques can be applied to various types of machine learning tasks, including classification, regression, anomaly detection, and more.\n",
    "- **Adaptability:** They can be adapted to different learning algorithms and used in various domains, such as finance, healthcare, marketing, and image recognition.\n",
    "\n",
    "### Examples of Ensemble Techniques and Their Benefits\n",
    "\n",
    "1. **Bagging (Bootstrap Aggregating):**\n",
    "   - Reduces variance by averaging predictions from multiple models trained on different subsets of the data.\n",
    "   - Example: Random Forests improve accuracy and robustness over individual decision trees.\n",
    "\n",
    "2. **Boosting:**\n",
    "   - Reduces bias by sequentially training models to correct the errors of previous models.\n",
    "   - Example: AdaBoost and Gradient Boosting can significantly enhance the performance of weak learners.\n",
    "\n",
    "3. **Stacking:**\n",
    "   - Combines multiple base models and uses a meta-model to make the final prediction, leveraging the strengths of each base model.\n",
    "   - Example: Using a combination of logistic regression, decision trees, and neural networks to improve predictive performance.\n",
    "\n",
    "### Summary\n",
    "\n",
    "Ensemble techniques provide a powerful approach to improving the performance and robustness of machine learning models. By leveraging the strengths of multiple models and compensating for their individual weaknesses, ensembles achieve better accuracy, generalization, and stability. They are versatile, flexible, and capable of handling complex datasets and problems, making them an essential tool in the machine learning practitioner's toolkit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a7d1c1-7f91-4ccb-8ed1-38f7c6386119",
   "metadata": {},
   "source": [
    "**Q6. Are ensemble techniques always better than individual models?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdfffa0-3faf-40a6-8faa-09f5c15dd673",
   "metadata": {},
   "source": [
    "**ANSWER:--------**\n",
    "\n",
    "\n",
    "While ensemble techniques often provide superior performance compared to individual models, they are not always the best choice in every situation. Here are some key considerations that determine whether ensemble techniques are the best approach:\n",
    "\n",
    "### When Ensemble Techniques Are Better:\n",
    "\n",
    "1. **Accuracy and Performance:**\n",
    "   - **Improved Accuracy:** Ensembles generally improve the accuracy of predictions by combining multiple models, which can average out errors.\n",
    "   - **Reduction of Overfitting:** Methods like bagging (e.g., Random Forests) reduce overfitting by training on different subsets of the data.\n",
    "\n",
    "2. **Robustness and Stability:**\n",
    "   - **Handling Noisy Data:** Ensembles are more robust to noise and outliers because the combined predictions of multiple models mitigate the impact of anomalies.\n",
    "   - **Reduction of Variance:** Ensembles like bagging reduce the variance of predictions, leading to more stable models.\n",
    "\n",
    "3. **Complex Problems:**\n",
    "   - **High-Dimensional Data:** Ensembles can effectively handle high-dimensional and complex datasets by leveraging the strengths of different models.\n",
    "   - **Non-linear Relationships:** They can capture complex non-linear relationships that might be missed by a single model.\n",
    "\n",
    "4. **Generalization:**\n",
    "   - **Bias-Variance Trade-off:** By combining models that might have high variance or high bias, ensembles achieve a better balance and improve generalization to new data.\n",
    "\n",
    "### When Individual Models Might Be Better:\n",
    "\n",
    "1. **Simplicity and Interpretability:**\n",
    "   - **Ease of Interpretation:** Individual models, like decision trees or linear regression, are often easier to interpret and explain to stakeholders.\n",
    "   - **Simplicity:** Simple models are easier to implement, understand, and maintain.\n",
    "\n",
    "2. **Computational Efficiency:**\n",
    "   - **Resource Constraints:** Training and maintaining an ensemble of models can be computationally intensive and time-consuming compared to a single model.\n",
    "   - **Faster Predictions:** Single models generally make predictions faster, which can be crucial in real-time applications.\n",
    "\n",
    "3. **Overfitting Risk:**\n",
    "   - **Risk of Overfitting:** Ensembles, especially complex ones like Gradient Boosting with many iterations, can still overfit if not properly regularized.\n",
    "   - **Noisy Data:** In cases where the data is very noisy, ensembles might exacerbate overfitting to noise.\n",
    "\n",
    "4. **Availability of Data:**\n",
    "   - **Small Datasets:** For very small datasets, the benefit of ensembles might be limited, and simpler models could perform just as well or better.\n",
    "\n",
    "5. **Specific Applications:**\n",
    "   - **Specialized Models:** In some cases, specialized individual models might be better suited for a particular task. For example, Convolutional Neural Networks (CNNs) are particularly effective for image recognition tasks and might not benefit as much from ensembling with other models.\n",
    "\n",
    "### Examples of Considerations:\n",
    "\n",
    "1. **Random Forest vs. Single Decision Tree:**\n",
    "   - Random Forests often outperform single decision trees due to reduced overfitting and higher accuracy.\n",
    "   - However, a single decision tree is easier to interpret and visualize, which can be important for understanding model decisions.\n",
    "\n",
    "2. **Gradient Boosting vs. Logistic Regression:**\n",
    "   - Gradient Boosting can provide higher accuracy on complex tasks but is computationally expensive and harder to interpret.\n",
    "   - Logistic Regression is fast, simple, and interpretable, making it a better choice for problems where interpretability and speed are crucial.\n",
    "\n",
    "### Summary:\n",
    "\n",
    "Ensemble techniques generally offer improved performance, robustness, and generalization but come at the cost of increased complexity and computational resources. Whether ensembles are the best choice depends on the specific requirements of the task, including the need for accuracy, interpretability, computational efficiency, and the characteristics of the dataset. It's essential to consider these factors and evaluate both ensemble and individual models to determine the best approach for a given problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9484f9ea-17d5-413d-8fae-08f078564051",
   "metadata": {},
   "source": [
    "**Q7. How is the confidence interval calculated using bootstrap?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0758c3-3b91-4ea0-a582-36cc77157657",
   "metadata": {},
   "source": [
    "**ANSWER:--------**\n",
    "\n",
    "\n",
    "Calculating a confidence interval using bootstrap involves resampling from the original dataset to estimate the variability of a statistic (such as the mean, median, or any other measure of interest) and then using the distribution of these bootstrap samples to determine the interval within which the true population parameter is likely to fall. Here’s a step-by-step outline of how this is typically done:\n",
    "\n",
    "### Steps to Calculate Confidence Interval Using Bootstrap:\n",
    "\n",
    "1. **Original Data:**\n",
    "   - Start with your original dataset, denoted as \\( D = \\{x_1, x_2, ..., x_n\\} \\), where \\( x_i \\) represents each data point.\n",
    "\n",
    "2. **Resampling (Bootstrap Samples):**\n",
    "   - Generate multiple bootstrap samples by randomly sampling with replacement from the original dataset. Each bootstrap sample \\( D^*_i \\) will have the same size as the original dataset \\( n \\), but may contain duplicate instances.\n",
    "\n",
    "3. **Calculate Statistic:**\n",
    "   - Compute the statistic of interest (e.g., mean, median, standard deviation, etc.) for each bootstrap sample \\( D^*_i \\). Let's denote this statistic as \\( \\theta^*_i \\).\n",
    "\n",
    "4. **Bootstrap Distribution:**\n",
    "   - Collect all the computed statistics \\( \\theta^*_1, \\theta^*_2, ..., \\theta^*_B \\) from the bootstrap samples, where \\( B \\) is the number of bootstrap samples generated.\n",
    "\n",
    "5. **Estimate Confidence Interval:**\n",
    "   - Sort the bootstrap statistics \\( \\theta^*_1, \\theta^*_2, ..., \\theta^*_B \\) in ascending order.\n",
    "   - Choose the appropriate percentiles from this sorted list to form the confidence interval. For example, for a 95% confidence interval, you would typically choose the 2.5th percentile (lower bound) and the 97.5th percentile (upper bound).\n",
    "\n",
    "### Example Calculation:\n",
    "\n",
    "Let's say we want to calculate a 95% confidence interval for the mean of a dataset using bootstrap:\n",
    "\n",
    "1. **Original Data:** Suppose our original dataset \\( D \\) has \\( n \\) observations.\n",
    "\n",
    "2. **Bootstrap Sampling:**\n",
    "   - Generate \\( B \\) bootstrap samples \\( D^*_1, D^*_2, ..., D^*_B \\) by sampling with replacement from \\( D \\).\n",
    "\n",
    "3. **Compute Sample Means:**\n",
    "   - For each bootstrap sample \\( D^*_i \\), compute the sample mean \\( \\theta^*_i \\).\n",
    "\n",
    "4. **Bootstrap Distribution:**\n",
    "   - Collect all the sample means \\( \\theta^*_1, \\theta^*_2, ..., \\theta^*_B \\).\n",
    "\n",
    "5. **Calculate Confidence Interval:**\n",
    "   - Sort the sample means \\( \\theta^*_1, \\theta^*_2, ..., \\theta^*_B \\).\n",
    "   - Calculate the 2.5th and 97.5th percentiles of this sorted list to find the lower and upper bounds of the 95% confidence interval.\n",
    "\n",
    "\n",
    "### Summary:\n",
    "\n",
    "Bootstrap resampling provides a robust method to estimate confidence intervals for statistics without assuming normality or specific distributions in the data. By generating multiple bootstrap samples, computing the desired statistic for each sample, and then using the distribution of these statistics, you can estimate the variability and uncertainty associated with the population parameter of interest, such as the mean, median, or variance. Adjust the confidence level by choosing appropriate percentiles from the bootstrap distribution based on your desired confidence level (e.g., 90%, 95%, 99%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18ced332-bce3-406c-a6f6-b9bb98612a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% Confidence Interval for the Mean: [8.4, 15.4]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example data (replace with your actual data)\n",
    "data = np.array([3, 5, 7, 9, 11, 13, 15, 17, 19, 21])\n",
    "\n",
    "# Number of bootstrap samples\n",
    "B = 1000\n",
    "\n",
    "# Function to generate bootstrap samples\n",
    "def generate_bootstrap_samples(data, B):\n",
    "    n = len(data)\n",
    "    bootstrap_samples = [np.random.choice(data, size=n, replace=True) for _ in range(B)]\n",
    "    return bootstrap_samples\n",
    "\n",
    "# Function to calculate mean from bootstrap samples\n",
    "def calculate_bootstrap_means(bootstrap_samples):\n",
    "    return np.array([np.mean(sample) for sample in bootstrap_samples])\n",
    "\n",
    "# Generate bootstrap samples\n",
    "bootstrap_samples = generate_bootstrap_samples(data, B)\n",
    "\n",
    "# Calculate means from bootstrap samples\n",
    "bootstrap_means = calculate_bootstrap_means(bootstrap_samples)\n",
    "\n",
    "# Calculate confidence interval (95% in this example)\n",
    "confidence_interval = np.percentile(bootstrap_means, [2.5, 97.5])\n",
    "\n",
    "print(f\"95% Confidence Interval for the Mean: [{confidence_interval[0]}, {confidence_interval[1]}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec48f64-7931-4b5d-b145-c3213b5525d1",
   "metadata": {},
   "source": [
    "**Q8. How does bootstrap work and What are the steps involved in bootstrap?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b29cc2-decc-4b11-9f76-aeed90a91aa6",
   "metadata": {},
   "source": [
    "**ANSWER:--------**\n",
    "\n",
    "\n",
    "Bootstrap is a powerful statistical method used to estimate the sampling distribution of a statistic by resampling with replacement from the original dataset. It's particularly useful when the underlying distribution of the data is unknown or when traditional statistical methods are not applicable. Here are the fundamental steps involved in bootstrap:\n",
    "\n",
    "### Steps Involved in Bootstrap:\n",
    "\n",
    "1. **Original Data:**\n",
    "   - Start with your original dataset \\( D = \\{x_1, x_2, ..., x_n\\} \\), where \\( x_i \\) represents each data point.\n",
    "\n",
    "2. **Sampling with Replacement:**\n",
    "   - Generate multiple bootstrap samples by randomly sampling \\( n \\) observations (with replacement) from the original dataset \\( D \\), where \\( n \\) is the size of the original dataset.\n",
    "   - Each bootstrap sample \\( D^*_i \\) will have the same size \\( n \\) as the original dataset but may contain duplicate instances.\n",
    "\n",
    "3. **Calculate Statistic:**\n",
    "   - Compute the statistic of interest (e.g., mean, median, standard deviation, etc.) for each bootstrap sample \\( D^*_i \\). Let's denote this statistic as \\( \\theta^*_i \\).\n",
    "\n",
    "4. **Bootstrap Distribution:**\n",
    "   - Collect all the computed statistics \\( \\theta^*_1, \\theta^*_2, ..., \\theta^*_B \\) from the bootstrap samples, where \\( B \\) is the number of bootstrap samples generated.\n",
    "\n",
    "5. **Estimate Population Parameter:**\n",
    "   - Use the distribution of these bootstrap statistics \\( \\theta^*_1, \\theta^*_2, ..., \\theta^*_B \\) to estimate the population parameter.\n",
    "   - Typically, the mean or median of these bootstrap statistics provides an estimate of the population parameter.\n",
    "   - Confidence intervals can be constructed using percentiles of the bootstrap statistics.\n",
    "\n",
    "### Example Scenario:\n",
    "\n",
    "Let's illustrate the steps with a simple example of estimating the mean of a dataset using bootstrap:\n",
    "\n",
    "#### Example Calculation:\n",
    "\n",
    "Suppose we have the following dataset:\n",
    "\\[ D = \\{3, 5, 7, 9, 11, 13, 15, 17, 19, 21\\} \\]\n",
    "\n",
    "1. **Original Data:**\n",
    "   - \\( D \\) is our original dataset.\n",
    "\n",
    "2. **Bootstrap Sampling:**\n",
    "   - Generate multiple bootstrap samples by sampling with replacement from \\( D \\).\n",
    "   - For example, one bootstrap sample might be \\( D^*_1 = \\{7, 3, 9, 15, 17, 5, 19, 3, 15, 7\\} \\).\n",
    "   - Repeat this process to create \\( B \\) bootstrap samples.\n",
    "\n",
    "3. **Calculate Bootstrap Means:**\n",
    "   - Compute the mean for each bootstrap sample \\( D^*_i \\).\n",
    "   - For instance, calculate \\( \\theta^*_i \\) for each \\( D^*_i \\).\n",
    "\n",
    "4. **Bootstrap Distribution:**\n",
    "   - Gather all the means \\( \\theta^*_1, \\theta^*_2, ..., \\theta^*_B \\) into a distribution.\n",
    "\n",
    "5. **Estimate Population Parameter:**\n",
    "   - Compute the mean of the bootstrap means as an estimate of the population mean.\n",
    "   - Construct a confidence interval using percentiles of the bootstrap means to estimate uncertainty.\n",
    "\n",
    "\n",
    "### Summary:\n",
    "\n",
    "Bootstrap resampling is a versatile and robust method for estimating the sampling distribution of a statistic without relying on theoretical distributions. By generating multiple bootstrap samples from the original data, computing the statistic of interest for each sample, and analyzing the distribution of these statistics, you can estimate population parameters and quantify uncertainty through confidence intervals. This makes bootstrap a valuable tool in statistical inference and hypothesis testing, especially in cases where traditional methods are not applicable or when the underlying distribution of the data is unknown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a429f0d-c4ca-4eaa-9ff6-a05862829bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data: [ 3  5  7  9 11 13 15 17 19 21]\n",
      "Bootstrap Means: [12.2 10.2 11.4 11.2  9.  12.4 12.6 13.2 13.2 13.  11.2 10.4  9.8 14.6\n",
      " 15.2 14.2 14.2 13.8  8.2 13.  11.  11.  13.4 12.2 10.8 12.   9.  13.8\n",
      " 11.8 11.8 15.  12.4 13.2  8.8  8.6 11.6 10.2 12.6 13.4 15.6 12.2  8.4\n",
      " 10.8 12.4 12.   8.2 15.  11.2 10.4 11.4  9.8 13.  14.8 11.  12.2 12.8\n",
      "  9.4 13.6 12.2  9.2 12.8 12.  12.4 13.8 11.  15.6 14.  18.2 10.  10.6\n",
      " 13.4 10.6 11.8 11.  12.6 12.2 11.4 13.2 12.   9.8 10.2 12.6 11.8 12.8\n",
      " 13.2  9.4 15.   9.   9.  15.6  8.6 10.4 10.8 14.4 15.4 14.8 13.2 10.4\n",
      " 12.  12.  12.2 10.6 14.2 12.4 13.   8.8 14.8 12.8 10.4 13.2 13.6 12.6\n",
      " 13.  12.8 14.4 11.4 12.  10.6 13.2  6.6 11.4 14.6 13.2 14.8 11.4 15.\n",
      " 14.6 14.2 15.6  9.4 12.4 11.8 13.6 13.4 12.6 10.6 12.2 12.2 10.6 13.\n",
      "  9.8 11.6 14.2 14.2 12.2 10.6 13.  12.8 12.  12.  13.6 10.  12.6 13.6\n",
      " 11.2 14.6 11.8 11.8 14.2 11.6 10.4 13.8 11.2 11.6 11.8 13.  14.4 10.2\n",
      " 13.6 10.4  9.2 13.4 10.6 11.  10.  11.4 15.  10.6 13.2 11.8 13.6 11.8\n",
      " 10.  12.8 12.6 13.6 10.6  9.8 11.6 12.  13.6  8.4 13.  12.2 12.4 10.4\n",
      " 13.2 11.4 12.   9.8 13.4  9.2 13.4 16.2  9.6 10.4 10.8 12.6 14.8 10.4\n",
      " 12.6 10.2 15.  10.8 10.8 11.4 11.  14.4 11.6 15.2 10.4 12.4 12.4 11.2\n",
      " 13.8 11.6 12.4  7.8 10.8 15.  12.2 11.4 15.4  9.4  9.8 12.8 10.6 13.\n",
      " 13.  15.2 12.6 10.2 14.8 15.4 11.8 11.4 14.2 13.6 12.8 14.4 11.8 16.2\n",
      " 12.2 11.4 10.4 10.  12.6  9.6 11.4 12.  11.2 12.2 11.6 11.  11.4 10.4\n",
      "  8.2  9.  12.8 10.8 15.4 12.  11.4  8.6  9.  12.2 12.2 10.6 12.6 10.6\n",
      " 13.2 14.8 12.6 12.  14.6 14.6 10.8 11.6  6.8  9.2 12.2 13.2 13.8 13.\n",
      " 11.4 11.  13.4 11.4 15.6 11.   8.6 10.2  8.2 13.  15.6 10.2 13.8 12.\n",
      " 10.  10.8 11.4 10.  10.6 11.8 12.  11.6 13.2 10.2 11.  13.8 12.2 12.4\n",
      "  9.6  8.4 11.2 12.  13.8  8.8 14.6 12.8 10.6 14.  12.8 11.2 12.6 10.2\n",
      " 13.2 14.  11.2 11.  14.6 13.8 12.8 12.4 14.4 12.4 11.6  9.  13.  10.8\n",
      " 11.4 14.4 10.8  8.8 12.8 12.6  9.2 10.2  8.4  7.6 11.  11.4 11.  11.\n",
      " 11.4 11.8 13.2 11.2 16.2 11.6 14.2 11.2 10.4 13.8 11.8 15.  10.8 12.\n",
      " 12.8 10.2 11.2 10.8 13.6 11.  14.6 14.  13.  10.8 14.  10.6 12.6 14.6\n",
      "  9.8 12.4 13.2 12.6 12.8 10.6 11.8  9.4 12.2 11.  12.4 10.  13.2 12.8\n",
      " 15.4 12.6  9.6 14.4 10.6 11.8 12.6 12.2 14.  13.6 10.8  8.4 11.6 15.6\n",
      "  9.8 13.  15.2 11.8 12.6 11.4 12.2  9.6 11.4 12.6 12.2 11.4 13.6 13.8\n",
      " 13.4  9.  12.6  8.6 11.6 10.2 12.  11.8 10.2 11.4 13.8 11.6 10.8  9.6\n",
      " 12.6 16.2 14.2 13.6 13.  10.8 11.8 11.8 14.   9.8 13.6 14.  14.4 13.2\n",
      "  9.8 11.8 11.6 13.  11.8 15.2 11.8 12.4 12.4 13.  10.4  8.6 12.8 13.2\n",
      " 12.6 12.8 16.4 12.   8.6 15.6 12.4 10.4 14.6 12.8 10.2 15.   9.6 11.6\n",
      " 13.4 14.2 14.2 10.4 12.  10.6 11.6 14.4 12.  10.8 12.4  7.4 13.  12.8\n",
      " 14.   8.  10.8 16.  10.6 14.  12.8 13.8 11.2 10.8 11.6  9.8 12.2 10.8\n",
      " 11.2 13.6 14.  12.   9.8 14.  10.8 13.4 10.4 11.4 14.  11.6 11.6 11.4\n",
      " 15.  14.2  9.8 11.  12.8 10.4 12.2 11.8 14.2 11.8 13.8 13.4 10.4 13.2\n",
      " 15.  15.4 11.4 13.8 13.2 12.8 14.  10.4 15.2 11.2 11.8 11.8 10.2 10.2\n",
      " 11.  12.  12.4 13.  11.8 12.2  8.4 13.8 12.  15.  11.4  8.6  9.2 10.4\n",
      " 10.2 13.   9.2 11.8 10.4 10.6 10.4 10.2 11.8  7.6 12.  11.  12.  12.2\n",
      " 12.8 12.6 12.6  9.8 12.   8.6 12.2 11.8 10.6 12.4 13.6 12.  10.4 13.\n",
      " 16.8 11.2 12.4 14.2 12.2 11.8 14.4 11.6 14.2 12.8 10.   9.   9.6 13.6\n",
      " 12.8 11.2 12.6 13.4 10.  12.2 12.2 11.  13.8 11.6 14.2 14.2 10.   9.4\n",
      " 13.6 12.2  9.4 11.  13.6 11.8 14.8 12.8  9.4 13.  13.2  9.  13.6 11.6\n",
      " 14.4 14.6 11.4 14.4  9.8 12.6 12.  12.2 13.4 12.2 13.2 15.  11.6 12.4\n",
      " 11.4 13.6 11.2 11.4 14.8 10.  12.4 11.4 14.  13.  13.6 15.4 15.  10.\n",
      " 11.6 10.4 10.  13.  10.  12.2 15.6 12.6 11.2 16.  11.4 10.2  9.  14.\n",
      " 15.4 10.6 11.8  9.8 11.4  9.6 13.6 12.8  9.8 14.4  9.  12.2 11.8 12.8\n",
      " 11.  12.6 10.8 15.6 12.  14.8 12.4 11.8 13.2 10.  12.2 13.6 12.  14.6\n",
      " 12.8 12.2 12.4 10.2 12.4 13.  10.4 10.  12.6  9.8 11.4 12.2 13.  11.2\n",
      " 13.6 15.8 10.4 10.6  8.  12.  14.  11.2 13.  12.6 14.2 12.8 13.8 11.4\n",
      " 11.  11.2 10.  12.  11.4 13.8  9.2 11.8 13.6 10.4 12.2 11.4 12.2 12.2\n",
      " 13.2 10.8  9.8 12.4 12.6 12.  11.8 14.8 11.2 11.6 11.8 12.4 12.8 12.8\n",
      " 12.4 12.  11.2 14.6 10.8 12.4 13.  11.6 14.2 10.8 12.   8.8 13.4 10.4\n",
      " 11.2 14.  11.   7.8 10.8 11.  13.  10.8 10.6  8.8  9.6 12.6 14.6 11.4\n",
      " 12.4 10.8 11.  12.6 12.2 12.  10.4 11.2 11.4 11.4 11.2 12.  11.6 12.4\n",
      " 13.4 12.8  9.8 15.6 11.6 12.6 12.4 11.6  9.8 12.2 11.4 12.6 10.6 10.8\n",
      " 13.2 13.8 11.4 13.4 15.  13.  13.2  9.  12.6 11.8 11.8 10.  10.6 12.6\n",
      " 11.4 11.2  8.  11.6 10.8 10.8 11.  11.4 12.6 13.  12.8 12.  11.8 14.8\n",
      " 13.4 11.4  9.   9.4 10.4 12.  13.4 14.4 13.2 13.  11.2  9.6  9.2 11.6\n",
      " 10.  11.4  6.  13.   8.8 11.6 12.  10.6 12.4 11.2 11.6  7.  12.  13.\n",
      " 12.  14.4 12.8 13.6 12.   9.8  7.8 11.  14.6 12.  11.6 11.2 12.   8.2\n",
      " 11.6 11.6 13.8 11.6 10.2 12.2 10.4 10.4 10.  13.4  8.  10.6 12.8 12.4\n",
      " 13.6 12.6 12.  14.  14.4 14.4 12.4 12.2 10.  11.2 15.4 13.8 13.6 11.2\n",
      " 13.8 11.2  9.8 12.4 11.8 12.4 12.2  7.  11.6 13.6 14.2 15.6 10.8 11.6\n",
      "  8.6 12.4 12.6 11.4 10.2 10.2 13.6 15.4  9.  11.2 12.6  9.8 12.4 11.4\n",
      " 12.8 12.2 15.   8.8 14.6 10.4  9.4 13.  10.6 10.8 13.6  8.4 13.  14.\n",
      " 14.  13.8 11.2 13.   9.6 10.8 10.4 10.2  8.  11.  12.6 10.6 12.   8.4\n",
      " 14.2 11.4 14.6 14.2  8.2  8.8 11.6 14.  13.8 12.6 14.  13.2 10.6 13.4\n",
      "  8.6 11.2 14.2 13.2  9.6 10.8]\n",
      "95% Confidence Interval for the Mean: [8.4, 15.4]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example data (replace with your actual data)\n",
    "data = np.array([3, 5, 7, 9, 11, 13, 15, 17, 19, 21])\n",
    "\n",
    "# Number of bootstrap samples\n",
    "B = 1000\n",
    "\n",
    "# Function to generate bootstrap samples\n",
    "def generate_bootstrap_samples(data, B):\n",
    "    n = len(data)\n",
    "    bootstrap_samples = [np.random.choice(data, size=n, replace=True) for _ in range(B)]\n",
    "    return bootstrap_samples\n",
    "\n",
    "# Function to calculate mean from bootstrap samples\n",
    "def calculate_bootstrap_means(bootstrap_samples):\n",
    "    return np.array([np.mean(sample) for sample in bootstrap_samples])\n",
    "\n",
    "# Generate bootstrap samples\n",
    "bootstrap_samples = generate_bootstrap_samples(data, B)\n",
    "\n",
    "# Calculate means from bootstrap samples\n",
    "bootstrap_means = calculate_bootstrap_means(bootstrap_samples)\n",
    "\n",
    "# Calculate confidence interval (95% in this example)\n",
    "confidence_interval = np.percentile(bootstrap_means, [2.5, 97.5])\n",
    "\n",
    "print(f\"Original Data: {data}\")\n",
    "print(f\"Bootstrap Means: {bootstrap_means}\")\n",
    "print(f\"95% Confidence Interval for the Mean: [{confidence_interval[0]}, {confidence_interval[1]}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3602eee8-e481-4c81-9183-a7c5ca4a0ab4",
   "metadata": {},
   "source": [
    "**Q9. A researcher wants to estimate the mean height of a population of trees. They measure the height of a\n",
    "sample of 50 trees and obtain a mean height of 15 meters and a standard deviation of 2 meters. Use\n",
    "bootstrap to estimate the 95% confidence interval for the population mean height.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b22d048-8747-4c9d-b73f-c64701761395",
   "metadata": {},
   "source": [
    "**ANSWER:--------**\n",
    "\n",
    "\n",
    "To estimate the 95% confidence interval for the population mean height of trees using bootstrap, we can follow these steps based on the information provided:\n",
    "\n",
    "Given data:\n",
    "- Sample size (\\( n \\)): 50 trees\n",
    "- Sample mean height (\\( \\bar{x} \\)): 15 meters\n",
    "- Sample standard deviation (\\( s \\)): 2 meters\n",
    "\n",
    "### Steps to Estimate the Confidence Interval Using Bootstrap:\n",
    "\n",
    "1. **Original Sample:**\n",
    "   - Assume the sample of tree heights is represented by \\( D = \\{x_1, x_2, ..., x_{50}\\} \\).\n",
    "\n",
    "2. **Generate Bootstrap Samples:**\n",
    "   - Generate multiple bootstrap samples by resampling with replacement from the original sample \\( D \\).\n",
    "   - Each bootstrap sample \\( D^*_i \\) will have 50 observations, sampled with replacement from \\( D \\).\n",
    "\n",
    "3. **Calculate Bootstrap Mean Heights:**\n",
    "   - Compute the mean height for each bootstrap sample \\( D^*_i \\).\n",
    "\n",
    "4. **Bootstrap Distribution:**\n",
    "   - Gather all the bootstrap means \\( \\bar{x}^*_1, \\bar{x}^*_2, ..., \\bar{x}^*_B \\).\n",
    "\n",
    "5. **Estimate Confidence Interval:**\n",
    "   - Calculate the 95% confidence interval using percentiles of the bootstrap distribution of means.\n",
    "\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "- **Original Sample Data:** Simulated using a normal distribution for demonstration purposes. Replace `original_sample` with your actual data if available.\n",
    "  \n",
    "- **Bootstrap Sampling:** `generate_bootstrap_samples` function generates 1000 bootstrap samples from the original sample.\n",
    "\n",
    "- **Bootstrap Means:** `calculate_bootstrap_means` computes the mean height for each bootstrap sample.\n",
    "\n",
    "- **Confidence Interval:** `np.percentile` calculates the 95% confidence interval from the bootstrap means.\n",
    "\n",
    "### Interpretation:\n",
    "\n",
    "In the output, you will see:\n",
    "- The original sample mean height.\n",
    "- The array of bootstrap means, which represent the distribution of sample means obtained through bootstrap resampling.\n",
    "- The 95% confidence interval for the population mean height of trees, estimated from the bootstrap distribution of means.\n",
    "\n",
    "This approach allows you to estimate the uncertainty around the population mean height based on the given sample, providing a range within which the true population mean height is likely to fall with 95% confidence. Adjust the number of bootstrap samples (`B`) based on computational resources and desired precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c3216e6-b574-4028-a14f-a3ff5cfbacd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sample Mean: 15 meters\n",
      "Bootstrap Means: [15.18002651 15.52785049 15.52147826 15.17880167 15.54041639 15.15036961\n",
      " 14.87583157 15.20653403 15.25009801 14.96317335 15.58658165 15.50174785\n",
      " 14.89776862 14.940739   15.55331019 15.3854348  15.39905566 15.24088455\n",
      " 15.18632907 15.11036471 16.51884172 15.49211211 15.40448678 15.50024349\n",
      " 15.6193707  15.64792992 15.45952888 14.79544668 15.11137509 14.99059175\n",
      " 14.91823756 15.4229922  15.07176496 15.12085021 15.2063722  15.44652463\n",
      " 15.21233518 15.57634669 15.61244586 15.34081371 15.65627199 14.5736634\n",
      " 15.85701427 15.78026594 15.55689624 15.29041157 15.38454761 15.47699242\n",
      " 15.66137223 15.62598254 14.95490197 14.96273985 15.77636414 15.332117\n",
      " 15.71853892 15.61242788 14.61949516 14.97899621 15.60693227 15.3829381\n",
      " 15.52161615 15.53837448 15.25944428 15.30257201 15.69449356 15.51987609\n",
      " 15.26085899 15.35171679 15.17875019 15.26969863 15.94746659 15.70068576\n",
      " 15.3025501  14.76461036 14.80673431 15.36315039 15.51401505 15.39754298\n",
      " 15.23184905 15.03673059 15.69447184 15.74794625 14.85499895 15.70271182\n",
      " 14.95535902 15.23136489 15.30100405 15.84578255 15.22509142 14.99542194\n",
      " 15.91064977 14.69428143 14.55947986 15.44290517 15.77398154 15.34812131\n",
      " 15.36376068 15.02803614 15.47362881 15.06984172 15.43156606 15.253796\n",
      " 15.59564296 15.11245699 15.20264145 15.08015164 15.58098194 15.07828743\n",
      " 15.11777369 15.46289772 15.14291459 15.25052921 15.18122086 15.36637149\n",
      " 14.86760374 15.32758913 15.39387799 15.5360993  15.14455757 15.10187376\n",
      " 15.03683016 15.49412052 15.80244552 14.95805623 15.19437895 15.33423808\n",
      " 15.22972997 14.98839331 14.7651174  15.08106277 15.60391698 15.18216463\n",
      " 15.33273302 14.87623125 14.93326558 15.39111347 14.94707792 14.96167548\n",
      " 15.03177665 15.14996839 14.68871178 15.36753093 15.48579282 15.06551284\n",
      " 15.26550157 15.28339017 15.26776181 15.89416713 15.44591332 15.12083256\n",
      " 14.9905208  14.7859394  14.68892043 15.64364153 14.90380211 15.79613243\n",
      " 15.53596048 15.49670717 15.04918612 14.9648726  15.39318057 15.48943677\n",
      " 14.91882537 14.92358862 15.08980225 15.58022594 15.41320878 15.280361\n",
      " 15.22524686 15.44254509 14.46708965 14.95732814 15.0266777  15.41569188\n",
      " 14.93908211 15.20818396 15.60305102 15.47794908 15.00425361 15.11632184\n",
      " 15.62242158 15.14891629 14.48306122 15.71476836 14.93256535 15.93358178\n",
      " 15.41329949 15.40440772 15.33842923 15.10197793 15.40771227 14.32157028\n",
      " 15.094912   15.33377496 15.42440937 15.05281218 15.86643413 15.20985553\n",
      " 15.16637262 15.09398301 15.35554903 15.64514158 15.42105168 15.50401015\n",
      " 15.08826544 15.19710612 15.23831462 15.09500474 15.41713879 15.07716971\n",
      " 15.10023896 15.21810166 14.97142187 15.28165418 15.00133429 14.87344336\n",
      " 15.41730283 15.64702502 15.63296472 15.12822682 15.1980226  15.30190609\n",
      " 15.54244069 14.95577845 15.63043064 15.12083717 15.27080965 15.55321604\n",
      " 14.63754183 15.25722592 15.59422247 15.32482851 15.6727303  15.15601814\n",
      " 14.97740333 15.12803491 15.20554179 14.50588217 15.0031124  15.61756981\n",
      " 14.78777854 15.19446639 15.246897   15.55863122 15.45567696 15.48178333\n",
      " 15.21283926 15.33182711 15.49243205 15.6312401  15.51167499 15.04333464\n",
      " 15.12575616 15.43932564 15.18168541 15.59062005 15.2284911  15.09720481\n",
      " 15.30301616 15.85021136 15.16781532 15.58119074 15.54521785 15.59495158\n",
      " 15.05398201 15.515106   15.71171946 15.26080192 15.47885801 15.23057178\n",
      " 14.82449381 15.34306962 15.21350095 15.04492355 15.23536382 14.62518662\n",
      " 15.54244221 15.36981528 15.20299843 15.82656318 14.78352256 15.33169319\n",
      " 14.78643849 15.635241   14.86594639 14.80815815 15.18857455 15.41520271\n",
      " 15.28331261 16.04840738 15.32052841 15.09394712 15.07804018 15.08633878\n",
      " 15.45522562 15.48768184 15.47350643 14.84428172 15.84080714 15.46148894\n",
      " 15.58011462 15.42149942 15.20887176 15.67268959 14.77929128 15.31626189\n",
      " 15.11170676 15.59043076 15.86758691 15.28821529 15.0366584  15.4015728\n",
      " 15.85779942 15.00072668 15.2132131  15.19601467 15.07207286 15.63819447\n",
      " 14.7046203  15.59201625 15.77017786 15.01885325 15.40132638 15.17727412\n",
      " 15.49839814 15.14961784 14.9623633  15.25423923 14.58158913 15.1310356\n",
      " 15.21191073 15.68878108 15.54404611 15.24776137 15.36339918 15.3753215\n",
      " 15.07987128 15.41317847 15.51220056 15.45159487 14.91784632 14.8871135\n",
      " 15.86245945 15.6280782  15.61241654 14.93817226 15.35412246 15.02057898\n",
      " 15.10581645 15.43893069 15.42560067 15.56227558 15.6804212  15.44736878\n",
      " 15.35437216 15.91929614 15.19742044 15.20752461 15.43937537 15.73795636\n",
      " 15.18538359 14.86721513 14.5223034  16.22566388 14.9715281  15.35318944\n",
      " 15.11861508 15.28206306 15.15382296 15.45377331 15.57193674 15.43271422\n",
      " 14.97192577 15.77367245 15.26196543 15.07719152 14.62169796 15.27212729\n",
      " 15.68049357 15.38069532 15.18017887 15.18034881 15.57519222 15.21575541\n",
      " 15.19114541 15.47732986 15.03938073 14.77366155 14.93363661 15.31976227\n",
      " 14.83899397 15.42208686 15.36493617 15.02774361 15.28689588 15.18204872\n",
      " 15.29826444 14.96953042 14.66409801 16.08155389 15.22455503 15.63853824\n",
      " 15.44520099 15.17784444 14.9063436  15.91259839 15.0136214  14.99416728\n",
      " 15.11876347 15.20970347 15.30349715 15.32184123 15.45505052 15.5153627\n",
      " 15.49221769 15.4815999  15.19661662 15.31669557 15.02112855 15.21447995\n",
      " 15.25283408 15.30510392 15.29122052 15.05714842 15.31990578 14.64787098\n",
      " 15.71631145 15.19047354 15.84051193 15.13750492 15.18907313 15.64535303\n",
      " 15.1390659  15.51898716 15.38114523 15.46320327 14.90437809 15.30037718\n",
      " 15.23394391 15.47638565 15.5387881  15.16096662 15.69404667 15.09643009\n",
      " 15.92400111 14.95309918 15.67499479 15.18728972 15.46630497 15.29372401\n",
      " 15.68156678 15.12086984 15.17894148 14.74704779 15.06855947 14.31416173\n",
      " 15.07850103 15.28968202 14.97514175 15.81154757 15.08812    15.27388312\n",
      " 14.97480859 15.33279663 15.76013166 15.21485466 14.76710416 15.11707068\n",
      " 15.6485943  15.45646089 15.47848551 15.03722388 15.01272109 15.03665864\n",
      " 15.37598281 15.73756209 15.43996978 15.41617005 15.30536281 15.65988533\n",
      " 15.65619456 15.41295504 15.78099897 14.60047935 14.9698359  15.36504769\n",
      " 15.48482466 15.42255354 15.28868485 14.72312563 15.60621403 15.46105498\n",
      " 15.78587559 14.46612946 15.64804443 15.72690279 14.88050964 15.00921937\n",
      " 15.6465175  15.59999938 15.01521316 14.8416532  15.29063158 15.42306951\n",
      " 15.35597256 15.52127865 15.49992106 15.45491485 14.9042532  15.13305291\n",
      " 15.19235003 14.94818287 15.09475076 15.41092984 16.0863776  15.40222804\n",
      " 14.94483665 15.23990538 15.33866404 15.19612959 15.19103476 15.53806284\n",
      " 14.99849056 15.49083174 15.76116598 14.9032599  15.57090005 15.61338267\n",
      " 15.59568949 15.58530955 15.05197354 15.27428317 15.40723502 15.18169211\n",
      " 15.36561795 15.46555442 15.69982258 15.42187841 15.65814216 15.55485039\n",
      " 14.7301841  15.32857142 15.13422841 15.38159199 15.54056593 15.56584913\n",
      " 15.56142982 15.46268006 15.24763747 15.29372541 15.52691272 15.30872628\n",
      " 15.35251348 14.79089521 15.27335604 15.52904071 15.01659206 16.00874455\n",
      " 15.73110449 15.74365331 15.02359659 15.19845201 15.21858259 15.00943295\n",
      " 15.7477904  15.32316983 15.84143197 15.52381734 15.04497701 15.56007689\n",
      " 15.38998921 15.25197399 14.9137571  15.11086153 15.29343291 14.98206302\n",
      " 14.96701725 14.98862509 15.21739535 15.56715419 15.2357394  15.21803925\n",
      " 15.63544988 15.50338688 15.67973125 16.19288613 15.075743   15.20784281\n",
      " 15.20662427 15.74218957 15.31247202 15.5666193  15.46657973 15.19178604\n",
      " 14.57852942 15.56698545 15.6702102  15.48718976 15.57442492 15.09156626\n",
      " 14.86872298 15.12405003 15.11323727 15.4380772  15.55261295 15.08051835\n",
      " 15.02152122 15.96207756 15.00037395 15.039048   15.48440403 15.48705792\n",
      " 15.15865986 15.70076197 15.00994535 15.16084013 15.02180863 15.55591931\n",
      " 15.22979075 15.33285886 14.79948303 15.31147983 15.33078728 15.82427996\n",
      " 15.92026369 15.3535593  15.49128207 15.43220622 15.29264457 15.70427092\n",
      " 14.9934277  15.52525486 15.05956585 15.11060146 15.32965962 15.18979817\n",
      " 15.38604058 15.68431191 14.9189485  15.128762   15.20658994 15.35634634\n",
      " 15.4890175  16.27393969 15.50591251 14.76879431 15.45771547 15.13091466\n",
      " 15.45099853 15.58924054 15.25664852 15.47976235 14.90820098 15.32399195\n",
      " 15.55328287 15.56368818 15.63572536 15.00285283 15.13257632 15.36949085\n",
      " 15.15541802 15.15436149 15.10537929 15.36688563 15.26326721 15.2966946\n",
      " 14.64522018 14.97372218 15.37495337 15.53534554 15.11138837 14.94170169\n",
      " 14.88420066 15.43703575 14.98453098 14.91504095 15.1920686  15.27998707\n",
      " 15.64705844 15.05668156 15.46629472 15.42924986 15.3056179  15.50448579\n",
      " 15.25869419 15.16411629 15.3572476  14.98655488 15.61355696 16.00079\n",
      " 15.66516589 15.26674987 15.15989699 14.84694994 15.2791314  15.98998679\n",
      " 15.23399773 14.9040376  15.52015844 15.49552324 15.43983334 15.20117444\n",
      " 15.41872232 15.88808308 15.19679658 15.62070249 14.85414119 15.2361784\n",
      " 15.68917756 15.48380193 14.57653298 15.44855104 15.01646227 15.33603456\n",
      " 15.16336594 15.47527019 15.24564551 15.46892268 15.69125374 15.37233752\n",
      " 15.76821173 15.28778336 15.18298912 15.7671209  14.99654842 15.48943894\n",
      " 15.567685   15.63834941 14.98101215 15.0933814  14.90598754 15.14168414\n",
      " 15.40466211 15.67378661 14.85955822 14.80245236 14.99371751 15.9714499\n",
      " 15.19990123 14.64409278 14.65785538 15.44669752 15.50987477 15.3947422\n",
      " 15.32149185 14.99199351 15.36306887 15.16627518 15.4275388  15.7916777\n",
      " 15.08968718 15.46148013 15.40809721 14.99039044 15.19239784 14.96408818\n",
      " 15.18376827 15.03626793 15.65058774 14.71066929 15.93660707 15.39798015\n",
      " 15.66714387 14.91961622 15.45905125 15.18011483 15.25850025 15.24576617\n",
      " 15.77554724 14.4316382  14.83845977 15.60843635 15.32101369 15.94639295\n",
      " 15.04763311 15.46016126 15.15208326 15.29193621 14.78642698 15.34600874\n",
      " 14.75867401 15.44908347 15.11624874 15.33091666 15.88703621 15.0041211\n",
      " 15.42315163 15.69429436 14.97922119 15.84690657 15.2451861  15.31814326\n",
      " 14.65002618 14.78185039 16.02877157 15.5160652  15.31417804 15.38652627\n",
      " 15.5149378  15.8039848  15.54682079 15.07510624 14.66988054 15.35787708\n",
      " 15.21709864 15.9177012  15.53024083 15.14232526 15.03687823 15.1390604\n",
      " 14.8915929  15.80998289 15.21483145 15.04098838 15.0412777  15.49814828\n",
      " 14.87179105 15.30679203 16.0926898  15.47072049 15.5165885  15.56381873\n",
      " 15.92610291 15.46145513 14.83502539 15.60955508 15.88002218 14.38449242\n",
      " 15.58255545 15.50492055 15.34528752 15.29816333 15.53298596 15.22090629\n",
      " 15.27551287 15.26859559 15.08450039 15.34409555 15.03061861 14.85909395\n",
      " 15.81541584 15.31529116 15.4170242  15.07913647 15.03663534 15.20455851\n",
      " 15.54477294 14.55458748 15.24988237 14.92766351 15.30005176 15.99879302\n",
      " 15.45787001 14.90992107 15.36119868 14.87387901 15.91166874 15.67909774\n",
      " 15.55411488 15.3273656  15.16767653 15.40406588 15.98799142 15.20308952\n",
      " 14.90321419 15.44670847 15.53769567 14.70967704 15.42724841 15.28127002\n",
      " 15.51703585 15.89406677 15.81818375 15.16801649 15.55271497 15.63481735\n",
      " 15.63008052 15.82560206 15.43265296 15.23076127 15.09800077 15.3521032\n",
      " 15.1519137  14.96739427 15.58352773 15.23586649 15.13141707 15.31406416\n",
      " 15.20679595 15.2849172  15.61828598 14.77195833 15.06193146 15.00666211\n",
      " 15.38122445 15.47286021 14.90928902 15.61394872 15.03840275 15.19884345\n",
      " 15.55687377 15.5189683  15.1438857  15.21481944 15.7114738  15.84850385\n",
      " 15.45278387 14.90682235 15.71774414 15.67592656 15.91405639 15.68404326\n",
      " 15.03414909 15.29522997 15.51965135 15.2158742  15.65333301 15.70430414\n",
      " 15.65646765 15.49687205 15.71642661 15.46431965 14.95963983 15.54424044\n",
      " 15.00465964 14.59289418 15.58309184 14.55363393 15.41511228 14.90390396\n",
      " 14.89401644 15.49130754 15.3018913  15.22337882 15.12412484 15.83475483\n",
      " 15.37375881 15.75217975 15.84626163 14.80634351 15.34897069 15.43579439\n",
      " 15.15241362 15.26169959 14.9453813  15.32374191 15.36360219 15.20566806\n",
      " 15.46101452 15.2445764  14.66202741 14.83548806 15.63760723 15.73360752\n",
      " 15.28551958 15.32337479 14.55649226 15.30567137 15.10729785 15.40479614\n",
      " 15.09716689 15.84470495 15.52684958 15.41906522 15.26580732 14.71451153\n",
      " 15.2374214  15.5106409  15.40532052 15.53292411 15.32725758 15.20671716\n",
      " 14.80326867 15.777906   15.15538218 15.64079249 15.22257972 15.02796102\n",
      " 14.91447518 15.59101988 15.9261199  15.2187941  15.23409406 15.30041697\n",
      " 15.54393788 15.07321728 14.79071481 14.96537478 15.4835778  15.34881361\n",
      " 15.10661199 15.43061076 14.94724923 15.56747896 15.16992041 15.00545371\n",
      " 15.41356164 14.8926895  14.70858152 15.57651723]\n",
      "95% Confidence Interval for the Mean Height: [14.647804709210806, 15.917741076864898] meters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Given data\n",
    "sample_mean = 15  # Sample mean height (in meters)\n",
    "sample_std = 2    # Sample standard deviation (in meters)\n",
    "n = 50             # Sample size\n",
    "\n",
    "# Original sample data (simulated for example)\n",
    "np.random.seed(0)  # For reproducibility\n",
    "original_sample = np.random.normal(loc=sample_mean, scale=sample_std, size=n)\n",
    "\n",
    "# Number of bootstrap samples\n",
    "B = 1000\n",
    "\n",
    "# Function to generate bootstrap samples\n",
    "def generate_bootstrap_samples(data, B):\n",
    "    n = len(data)\n",
    "    bootstrap_samples = [np.random.choice(data, size=n, replace=True) for _ in range(B)]\n",
    "    return bootstrap_samples\n",
    "\n",
    "# Function to calculate mean from bootstrap samples\n",
    "def calculate_bootstrap_means(bootstrap_samples):\n",
    "    return np.array([np.mean(sample) for sample in bootstrap_samples])\n",
    "\n",
    "# Generate bootstrap samples\n",
    "bootstrap_samples = generate_bootstrap_samples(original_sample, B)\n",
    "\n",
    "# Calculate means from bootstrap samples\n",
    "bootstrap_means = calculate_bootstrap_means(bootstrap_samples)\n",
    "\n",
    "# Calculate confidence interval (95% in this example)\n",
    "confidence_interval = np.percentile(bootstrap_means, [2.5, 97.5])\n",
    "\n",
    "print(f\"Original Sample Mean: {sample_mean} meters\")\n",
    "print(f\"Bootstrap Means: {bootstrap_means}\")\n",
    "print(f\"95% Confidence Interval for the Mean Height: [{confidence_interval[0]}, {confidence_interval[1]}] meters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0268047-417b-4294-9d92-606412a56568",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
